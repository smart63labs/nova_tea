# Guia de Uso: Funcionalidade de Web Scraping

## Vis√£o Geral

A funcionalidade de Web Scraping permite extrair conte√∫do de p√°ginas web e adicion√°-lo automaticamente √† Base de Conhecimento do sistema. O sistema utiliza uma abordagem h√≠brida que detecta automaticamente se o site √© est√°tico ou requer JavaScript, selecionando o scraper mais adequado.

## Como Usar

### 1. Acessar a Funcionalidade

1. Abra a aplica√ß√£o
2. Clique no bot√£o **"Configura√ß√µes"** no canto superior direito
3. Navegue para a aba **"Scraping"**

### 2. Inserir URLs

1. No campo **"URLs para Processar"**, digite as URLs que deseja processar
2. Insira **uma URL por linha**
3. O sistema mostra quantas URLs v√°lidas foram detectadas

**Exemplo**:
```
https://pt.wikipedia.org/wiki/Intelig√™ncia_artificial
https://www.python.org/doc/
https://docs.python.org/3/tutorial/
```

### 3. Selecionar Base de Conhecimento

1. No dropdown **"Base de Conhecimento"**, selecione a base onde os arquivos ser√£o adicionados
2. Se n√£o houver bases dispon√≠veis, crie uma na aba **"Bases de Conhecimento"**

### 4. Processar

1. Clique no bot√£o **"Processar URLs"**
2. O sistema iniciar√° o processamento em background

### 5. Acompanhar Progresso

Durante o processamento, voc√™ ver√°:

- **Barra de Progresso**: Mostra quantas URLs foram processadas (ex: 2/5)
- **URL Atual**: Exibe qual URL est√° sendo processada no momento
- **Status**: Indica o estado atual (Processando, Conclu√≠do, Erro, etc.)
- **Lista de Resultados**: Mostra o status de cada URL processada
  - ‚úÖ **Sucesso**: Nome do arquivo gerado e tipo de scraper usado
  - ‚ùå **Erro**: Mensagem de erro detalhada

### 6. Cancelar (Opcional)

- Durante o processamento, voc√™ pode clicar em **"Cancelar"** para interromper

### 7. Processar Novas URLs

- Ap√≥s conclus√£o, clique em **"Processar Novas URLs"** para iniciar nova tarefa

---

## Tipos de Scraper

### Scrapy (Sites Est√°ticos)

**Usado para**: Sites com HTML renderizado no servidor

**Caracter√≠sticas**:
- ‚ö° Muito r√°pido (ass√≠ncrono)
- üíæ Baixo consumo de recursos
- ‚è±Ô∏è Timeout: 10 segundos

**Exemplos**: Wikipedia, blogs, documenta√ß√£o t√©cnica, sites institucionais

### Playwright (Sites JavaScript)

**Usado para**: Sites com conte√∫do carregado via JavaScript (SPAs)

**Caracter√≠sticas**:
- üåê Renderiza navegador completo
- üîÑ Aguarda carregamento de JavaScript
- ‚è±Ô∏è Timeout: 30 segundos

**Exemplos**: Aplica√ß√µes React, Vue, Angular, sites modernos

---

## Limita√ß√µes

### T√©cnicas

- **M√°ximo de URLs simult√¢neas**: 5
- **Timeout**:
  - Sites est√°ticos: 10 segundos
  - Sites JavaScript: 30 segundos
- **Tamanho m√°ximo de arquivo**: 5MB por arquivo Markdown
- **Sites n√£o suportados**:
  - Sites com CAPTCHA
  - Sites que requerem autentica√ß√£o/login
  - Sites com prote√ß√£o anti-scraping agressiva

### Legais e √âticas

- ‚öñÔ∏è **Respeite os termos de uso** dos sites
- ü§ñ **Respeite robots.txt** (o sistema faz isso automaticamente)
- ‚è∞ **Evite scraping excessivo** do mesmo dom√≠nio
- üìú **Use apenas para fins educacionais** ou com permiss√£o

---

## Resolu√ß√£o de Problemas

### "Nenhuma URL v√°lida fornecida"

**Causa**: URLs com formato inv√°lido

**Solu√ß√£o**: Verifique se as URLs:
- Come√ßam com `http://` ou `https://` (ou adicione automaticamente)
- T√™m um dom√≠nio v√°lido (ex: `example.com`)
- N√£o cont√™m caracteres especiais inv√°lidos

### "Timeout ao acessar [URL]"

**Causa**: Site demorou muito para responder

**Solu√ß√£o**:
- Verifique se o site est√° online
- Tente novamente mais tarde
- Se persistir, o site pode ter prote√ß√£o anti-scraping

### "Erro ao processar [URL]"

**Causa**: V√°rios motivos poss√≠veis

**Solu√ß√£o**:
- Verifique a mensagem de erro espec√≠fica
- Confirme se o site est√° acess√≠vel em um navegador
- Verifique se o site n√£o requer login

### "Servi√ßo de scraping n√£o dispon√≠vel"

**Causa**: Depend√™ncias n√£o instaladas no backend

**Solu√ß√£o**:
- Verifique se as depend√™ncias foram instaladas:
  ```bash
  pip install scrapy playwright html2text beautifulsoup4 lxml
  playwright install chromium
  ```

---

## Formato dos Arquivos Gerados

Os arquivos Markdown gerados t√™m o seguinte formato:

```markdown
---
t√≠tulo: [T√≠tulo da P√°gina]
url: [URL Original]
data_extra√ß√£o: [Data e Hora]
fonte: Web Scraping
---

# [T√≠tulo da P√°gina]

**URL Original**: [URL]  
**Data de Extra√ß√£o**: [Data]

---

[Conte√∫do extra√≠do em Markdown]
```

---

## Boas Pr√°ticas

### ‚úÖ Recomendado

- Processar URLs de documenta√ß√£o p√∫blica
- Usar para criar base de conhecimento de conte√∫do educacional
- Processar poucas URLs por vez (1-5)
- Aguardar conclus√£o antes de processar novo lote

### ‚ùå N√£o Recomendado

- Fazer scraping de sites comerciais sem permiss√£o
- Processar centenas de URLs simultaneamente
- Fazer scraping repetido do mesmo site em curto per√≠odo
- Tentar fazer scraping de sites com CAPTCHA

---

## Exemplos de Uso

### Exemplo 1: Documenta√ß√£o T√©cnica

**URLs**:
```
https://docs.python.org/3/tutorial/
https://docs.python.org/3/library/
```

**Resultado**: 2 arquivos Markdown com conte√∫do da documenta√ß√£o Python

### Exemplo 2: Artigos Wikipedia

**URLs**:
```
https://pt.wikipedia.org/wiki/Intelig√™ncia_artificial
https://pt.wikipedia.org/wiki/Aprendizado_de_m√°quina
```

**Resultado**: 2 arquivos Markdown com conte√∫do dos artigos

### Exemplo 3: Blog Posts

**URLs**:
```
https://blog.example.com/post-1
https://blog.example.com/post-2
```

**Resultado**: 2 arquivos Markdown com conte√∫do dos posts

---

## Suporte

Se encontrar problemas:

1. Verifique os logs do backend (`debug.log`)
2. Confirme que as depend√™ncias est√£o instaladas
3. Teste com URLs simples primeiro (ex: Wikipedia)
4. Verifique se a base de conhecimento est√° acess√≠vel

---

## Pr√≥ximos Passos

Ap√≥s o scraping:

1. V√° para a aba **"Bases de Conhecimento"**
2. Selecione a base onde fez upload
3. Verifique se os arquivos aparecem na lista
4. Teste fazendo perguntas ao agente associado √† base
