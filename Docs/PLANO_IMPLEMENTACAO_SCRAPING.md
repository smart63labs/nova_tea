# Plano de Implementa√ß√£o: Funcionalidade de Scraping H√≠brida

## Objetivo

Implementar uma funcionalidade de web scraping que permita aos usu√°rios extrair conte√∫do de URLs e convert√™-lo automaticamente em arquivos Markdown para upload na Base de Conhecimento. A solu√ß√£o utilizar√° uma abordagem h√≠brida (Scrapy + Playwright) para maximizar compatibilidade e performance.

## Contexto do Projeto

O projeto ADK (Advanced Development Kit) √© uma aplica√ß√£o multi-agente baseada em Flask (backend) e React/TypeScript (frontend). A nova funcionalidade ser√° integrada como uma aba adicional na se√ß√£o "Configura√ß√µes" do sistema.

### Arquitetura Atual
- **Backend**: Flask (`flask_app/app.py`)
- **Frontend**: React + TypeScript (`frontend/src/App.tsx`)
- **Base de Conhecimento**: Google File Search Stores (API Gemini)
- **Configura√ß√µes**: Sistema de abas com m√∫ltiplas funcionalidades

---

## Proposta de Mudan√ßas

### 1. Backend - Infraestrutura de Scraping

#### üìÅ Nova Estrutura de Pastas

```
flask_app/
  services/
    scraping/
      __init__.py
      scraper_factory.py      # Factory pattern para sele√ß√£o de scraper
      scrapy_scraper.py       # Scraper para sites est√°ticos
      playwright_scraper.py   # Scraper para sites JavaScript
      detector.py             # Detector de tipo de site
      markdown_converter.py   # Conversor HTML ‚Üí Markdown
      config.py               # Configura√ß√µes de scraping
```

#### üìÑ [NOVO] `flask_app/services/scraping/__init__.py`

```python
"""
Servi√ßo de scraping h√≠brido (Scrapy + Playwright)
"""
from .scraper_factory import ScraperFactory

__all__ = ['ScraperFactory']
```

#### üìÑ [NOVO] `flask_app/services/scraping/detector.py`

**Responsabilidade**: Detectar se um site requer JavaScript (SPA) ou √© est√°tico

**L√≥gica**:
- Faz requisi√ß√£o HTTP simples
- Analisa presen√ßa de frameworks JS (React, Vue, Angular)
- Verifica se conte√∫do principal est√° no HTML inicial
- Retorna: `'static'` ou `'dynamic'`

#### üìÑ [NOVO] `flask_app/services/scraping/scrapy_scraper.py`

**Responsabilidade**: Scraping r√°pido de sites est√°ticos

**Caracter√≠sticas**:
- Usa Scrapy para performance m√°xima
- Extrai t√≠tulo, conte√∫do principal, metadados
- Timeout de 10 segundos
- Respeita robots.txt
- User-Agent customizado

#### üìÑ [NOVO] `flask_app/services/scraping/playwright_scraper.py`

**Responsabilidade**: Scraping de sites JavaScript/SPAs

**Caracter√≠sticas**:
- Usa Playwright em modo headless
- Aguarda carregamento completo do DOM
- Timeout de 30 segundos
- Extrai conte√∫do ap√≥s renderiza√ß√£o JavaScript
- Suporte a scroll infinito (opcional)

#### üìÑ [NOVO] `flask_app/services/scraping/markdown_converter.py`

**Responsabilidade**: Converter HTML extra√≠do para Markdown

**Caracter√≠sticas**:
- Usa biblioteca `html2text`
- Preserva formata√ß√£o (t√≠tulos, listas, links)
- Remove scripts, estilos e elementos desnecess√°rios
- Adiciona metadados (URL, data de extra√ß√£o)

#### üìÑ [NOVO] `flask_app/services/scraping/scraper_factory.py`

**Responsabilidade**: Orquestrar o processo de scraping

**Fluxo**:
1. Recebe URL(s)
2. Detecta tipo de site
3. Seleciona scraper apropriado (Scrapy ou Playwright)
4. Executa scraping
5. Converte para Markdown
6. Retorna conte√∫do + metadados

#### üìÑ [NOVO] `flask_app/services/scraping/config.py`

**Configura√ß√µes**:
```python
SCRAPY_SETTINGS = {
    'USER_AGENT': 'ADK-Scraper/1.0',
    'ROBOTSTXT_OBEY': True,
    'CONCURRENT_REQUESTS': 5,
    'DOWNLOAD_TIMEOUT': 10
}

PLAYWRIGHT_SETTINGS = {
    'HEADLESS': True,
    'TIMEOUT': 30000,
    'WAIT_UNTIL': 'networkidle'
}
```

---

### 2. Backend - API Endpoints

#### üìÑ [MODIFICAR] `flask_app/app.py`

Adicionar novos endpoints:

##### `POST /api/scraping/process`

**Entrada**:
```json
{
  "urls": ["https://example.com", "https://example2.com"],
  "store_name": "fileSearchStores/xyz123"
}
```

**Processamento**:
1. Valida URLs
2. Cria tarefa ass√≠ncrona para cada URL
3. Retorna `task_id` para acompanhamento

**Sa√≠da**:
```json
{
  "task_id": "uuid-123",
  "total_urls": 2,
  "status": "processing"
}
```

##### `GET /api/scraping/status/<task_id>`

**Sa√≠da**:
```json
{
  "task_id": "uuid-123",
  "status": "processing",
  "progress": {
    "completed": 1,
    "total": 2,
    "current_url": "https://example2.com"
  },
  "results": [
    {
      "url": "https://example.com",
      "status": "success",
      "filename": "example_com.md",
      "file_uri": "files/abc123"
    }
  ]
}
```

##### `POST /api/scraping/cancel/<task_id>`

Cancela processamento em andamento.

---

### 3. Frontend - Interface de Scraping

#### üìÑ [MODIFICAR] `frontend/src/App.tsx`

Adicionar nova aba "Scraping" no di√°logo de Configura√ß√µes:

**Localiza√ß√£o**: Dentro do `<Tabs>` de configura√ß√µes (linha ~686)

**Nova Aba**:
```tsx
<TabsTrigger value="scraping">
  <Globe className="h-4 w-4" /> Scraping
</TabsTrigger>
```

#### üìÑ [NOVO] `frontend/src/components/ScrapingTab.tsx`

**Componente Principal**

**Funcionalidades**:
1. **Entrada de URLs**
   - Textarea para m√∫ltiplas URLs (uma por linha)
   - Valida√ß√£o de formato de URL
   - Contador de URLs v√°lidas

2. **Sele√ß√£o de Base de Conhecimento**
   - Dropdown com stores dispon√≠veis
   - Op√ß√£o de criar nova base

3. **Bot√£o de Processar**
   - Inicia scraping
   - Mostra loading state

4. **Barra de Progresso**
   - Progresso em tempo real
   - URLs processadas / total
   - URL atual sendo processada

5. **Lista de Resultados**
   - ‚úÖ Sucesso: mostra nome do arquivo gerado
   - ‚ùå Erro: mostra mensagem de erro
   - üîó Link para visualizar arquivo na base

6. **Preview de Markdown**
   - Accordion com preview de cada arquivo gerado
   - Usa ReactMarkdown para renderiza√ß√£o

**Estados**:
```typescript
interface ScrapingState {
  urls: string[];
  selectedStore: string;
  taskId: string | null;
  status: 'idle' | 'processing' | 'completed' | 'error';
  progress: {
    completed: number;
    total: number;
    currentUrl: string;
  };
  results: ScrapingResult[];
}
```

#### üìÑ [NOVO] `frontend/src/hooks/useScrapingProgress.ts`

**Hook Customizado**

**Responsabilidade**: Polling de status da tarefa

**L√≥gica**:
- Faz polling a cada 2 segundos
- Atualiza estado de progresso
- Para quando tarefa completa ou erro
- Retorna fun√ß√£o de cancelamento

---

### 4. Integra√ß√£o com Base de Conhecimento

#### Fluxo de Upload Autom√°tico

1. **Scraping Completo** ‚Üí Gera arquivo `.md` tempor√°rio
2. **Upload para Google Files API** ‚Üí Retorna `file_uri`
3. **Adiciona √† Store** ‚Üí Vincula arquivo √† base selecionada
4. **Notifica Frontend** ‚Üí Atualiza lista de resultados

#### Tratamento de Erros

- **URL inv√°lida**: Retorna erro antes de processar
- **Timeout**: Marca como falha e continua pr√≥xima URL
- **Erro de upload**: Tenta novamente (3 tentativas)
- **Store n√£o encontrada**: Retorna erro e para processamento

---

## Depend√™ncias Necess√°rias

### Backend (Python)

Adicionar ao `requirements.txt`:

```txt
scrapy==2.11.0
playwright==1.40.0
html2text==2020.1.16
beautifulsoup4==4.12.0
lxml==4.9.3
```

### Instala√ß√£o do Playwright

```bash
pip install playwright
playwright install chromium
```

### Frontend (TypeScript)

J√° possui todas as depend√™ncias necess√°rias:
- `react-markdown` ‚úÖ
- `lucide-react` ‚úÖ

---

## Plano de Verifica√ß√£o

### 1. Testes Automatizados Backend

#### Teste de Detector

```bash
# Criar arquivo: flask_app/services/scraping/tests/test_detector.py
python -m pytest flask_app/services/scraping/tests/test_detector.py -v
```

**Casos de Teste**:
- Site est√°tico (Wikipedia) ‚Üí deve retornar `'static'`
- Site SPA (React app) ‚Üí deve retornar `'dynamic'`
- URL inv√°lida ‚Üí deve lan√ßar exce√ß√£o

#### Teste de Scrapy Scraper

```bash
python -m pytest flask_app/services/scraping/tests/test_scrapy_scraper.py -v
```

**Casos de Teste**:
- Extra√ß√£o de t√≠tulo e conte√∫do
- Timeout em site lento
- Respeito a robots.txt

#### Teste de Playwright Scraper

```bash
python -m pytest flask_app/services/scraping/tests/test_playwright_scraper.py -v
```

**Casos de Teste**:
- Extra√ß√£o de conte√∫do JavaScript
- Aguardar carregamento completo
- Timeout

#### Teste de Conversor Markdown

```bash
python -m pytest flask_app/services/scraping/tests/test_markdown_converter.py -v
```

**Casos de Teste**:
- Convers√£o de HTML simples
- Preserva√ß√£o de formata√ß√£o
- Remo√ß√£o de scripts/estilos

### 2. Testes de Integra√ß√£o

#### Teste End-to-End da API

```bash
# Criar arquivo: flask_app/tests/test_scraping_api.py
python -m pytest flask_app/tests/test_scraping_api.py -v
```

**Casos de Teste**:
- POST /api/scraping/process com URLs v√°lidas
- GET /api/scraping/status/<task_id>
- POST /api/scraping/cancel/<task_id>
- Upload autom√°tico para base de conhecimento

### 3. Testes Manuais (Frontend)

> [!IMPORTANT]
> **Pr√©-requisito**: Backend e Frontend devem estar rodando

#### Passos para Teste Manual:

1. **Iniciar Backend**
   ```bash
   cd C:\Users\88417646191\Documents\ADK
   python flask_app/app.py
   ```

2. **Iniciar Frontend**
   ```bash
   cd C:\Users\88417646191\Documents\ADK\frontend
   npm run dev
   ```

3. **Acessar Interface**
   - Abrir navegador em `http://localhost:8080` (ou porta indicada)
   - Clicar em "Configura√ß√µes"
   - Navegar para aba "Scraping"

4. **Testar Scraping de Site Est√°tico**
   - Inserir URL: `https://pt.wikipedia.org/wiki/Intelig%C3%AAncia_artificial`
   - Selecionar base de conhecimento existente
   - Clicar em "Processar"
   - **Resultado Esperado**: 
     - Barra de progresso aparece
     - Arquivo `wikipedia_inteligencia_artificial.md` √© criado
     - Preview mostra conte√∫do formatado
     - Arquivo aparece na aba "Base de Conhecimento"

5. **Testar Scraping de M√∫ltiplas URLs**
   - Inserir 3 URLs diferentes (uma por linha)
   - Processar
   - **Resultado Esperado**:
     - Progresso mostra "1/3", "2/3", "3/3"
     - Todos os arquivos s√£o criados
     - Lista de resultados mostra status de cada URL

6. **Testar Tratamento de Erro**
   - Inserir URL inv√°lida: `https://site-que-nao-existe-123456.com`
   - Processar
   - **Resultado Esperado**:
     - Erro √© exibido na lista de resultados
     - √çcone de erro (‚ùå) aparece
     - Mensagem de erro √© clara

7. **Testar Cancelamento**
   - Inserir 5 URLs
   - Iniciar processamento
   - Clicar em "Cancelar" ap√≥s 2 URLs processadas
   - **Resultado Esperado**:
     - Processamento para
     - URLs j√° processadas permanecem na lista
     - Status muda para "cancelado"

### 4. Verifica√ß√£o de Integra√ß√£o com Base de Conhecimento

#### Usando MCP Chrome DevTools

```bash
# Verificar console do navegador em http://localhost:8080
# Procurar por erros relacionados a upload de arquivos
```

**Passos**:
1. Fazer scraping de uma URL
2. Ir para aba "Base de Conhecimento"
3. Verificar se arquivo aparece na lista
4. Clicar no arquivo para visualizar
5. **Resultado Esperado**: Conte√∫do Markdown renderizado corretamente

---

## Limita√ß√µes e Boas Pr√°ticas

### Limita√ß√µes

1. **Rate Limiting**: M√°ximo de 5 URLs simult√¢neas para evitar sobrecarga
2. **Timeout**: 10s para Scrapy, 30s para Playwright
3. **Tamanho de Arquivo**: M√°ximo de 5MB por arquivo Markdown
4. **Sites Protegidos**: Sites com CAPTCHA ou autentica√ß√£o n√£o funcionar√£o

### Boas Pr√°ticas

1. **Respeitar robots.txt**: Scrapy configurado para obedecer
2. **User-Agent Identific√°vel**: `ADK-Scraper/1.0`
3. **Delay entre Requisi√ß√µes**: 1 segundo entre URLs do mesmo dom√≠nio
4. **Limpeza de Arquivos Tempor√°rios**: Deletar ap√≥s upload bem-sucedido

---

## Cronograma de Implementa√ß√£o

### Fase 1: Backend Infraestrutura (Estimativa: 2-3 horas)
- [ ] Criar estrutura de pastas
- [ ] Implementar detector de tipo de site
- [ ] Implementar Scrapy scraper
- [ ] Implementar Playwright scraper
- [ ] Implementar conversor Markdown
- [ ] Implementar factory pattern

### Fase 2: Backend API (Estimativa: 1-2 horas)
- [ ] Criar endpoints de scraping
- [ ] Implementar sistema de tarefas ass√≠ncronas
- [ ] Integrar com upload para base de conhecimento
- [ ] Adicionar logging e tratamento de erros

### Fase 3: Frontend Interface (Estimativa: 2-3 horas)
- [ ] Criar componente ScrapingTab
- [ ] Implementar formul√°rio de entrada
- [ ] Criar barra de progresso
- [ ] Implementar lista de resultados
- [ ] Adicionar preview de Markdown
- [ ] Integrar com API backend

### Fase 4: Testes (Estimativa: 2 horas)
- [ ] Escrever testes unit√°rios backend
- [ ] Escrever testes de integra√ß√£o
- [ ] Executar testes manuais frontend
- [ ] Validar integra√ß√£o com base de conhecimento

### Fase 5: Documenta√ß√£o (Estimativa: 30 minutos)
- [ ] Documentar API endpoints
- [ ] Criar guia de uso para usu√°rios
- [ ] Documentar limita√ß√µes

**Tempo Total Estimado**: 7-10 horas

---

## Pr√≥ximos Passos

1. ‚úÖ Revisar este plano com o usu√°rio
2. ‚è≥ Aguardar aprova√ß√£o
3. ‚è≥ Iniciar implementa√ß√£o Fase 1
4. ‚è≥ Testes incrementais a cada fase
5. ‚è≥ Deploy e valida√ß√£o final

---

## Observa√ß√µes Finais

- A solu√ß√£o h√≠brida garante **m√°xima compatibilidade** com diferentes tipos de sites
- O sistema de **detec√ß√£o autom√°tica** torna a experi√™ncia transparente para o usu√°rio
- A **integra√ß√£o nativa** com a base de conhecimento elimina passos manuais
- O **feedback em tempo real** melhora a experi√™ncia do usu√°rio

